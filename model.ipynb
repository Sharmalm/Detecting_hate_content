{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting hate content in songs"
      ],
      "metadata": {
        "id": "VYZyKc3REwOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am fine tuning a pretrained model for detecting hate speech."
      ],
      "metadata": {
        "id": "DlFBTVgA-a6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6dPmSZtUGjDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIq-0lamd6mB"
      },
      "outputs": [],
      "source": [
        "# install dependancies\n",
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHQjZ3Ds-hqo"
      },
      "outputs": [],
      "source": [
        "# import dependancies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neiEEJBg3j5H"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjjusceUXEaE"
      },
      "outputs": [],
      "source": [
        "# I am using this open source data for fine tuning pretrained model\n",
        "train = pd.read_csv('/content/drive/MyDrive/research/HackFS/train_comment.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtYr-EgWdmwp"
      },
      "outputs": [],
      "source": [
        "Dataset_train_original = train[[ \"comment_text\"  , \"toxic\"]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing dataset\n",
        "Dataset_train_original.head(100)"
      ],
      "metadata": {
        "id": "wjcG3d9VSXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjArFpjNkW2T"
      },
      "outputs": [],
      "source": [
        "# take sample train and test data from the original data for fine-tuning\n",
        "Dataset_train = Dataset_train_original.head(5000)  # sample data of 5000 rows\n",
        "Dataset_test = Dataset_train_original.tail(1000)  # sample data of 1000 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDJA9xOwkEYh"
      },
      "outputs": [],
      "source": [
        "Dataset_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCui0XjvnbKU"
      },
      "outputs": [],
      "source": [
        "# renaming the column of dataset\n",
        "Dataset_test = Dataset_test.rename(columns={'toxic': 'target'})\n",
        "Dataset_train = Dataset_train.rename(columns={'toxic': 'target'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_train"
      ],
      "metadata": {
        "id": "boTBRf-JTY_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process dataset"
      ],
      "metadata": {
        "id": "yDX7TdiDAGmn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds9kNGhCpBWI"
      },
      "source": [
        "Now encode the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4tiKZrWoD9E"
      },
      "outputs": [],
      "source": [
        "# Define tokenizer and model.\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "# define tokenizer and model\n",
        "# I am using \"toxic-bert\" as pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\" , from_pt = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qi8klFr0l5"
      },
      "outputs": [],
      "source": [
        "# Define label\n",
        "labels = np.array(Dataset_train[\"target\"])\n",
        "\n",
        "num_classes = 6  # Total number of classes\n",
        "labels = tf.one_hot(labels, num_classes)  # Convert labels to one-hot encoded vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skzJzfy7jbDu",
        "outputId": "ce9e4609-794c-44f2-c345-17816c3c9ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 6)\n"
          ]
        }
      ],
      "source": [
        "# label shape\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Wb6YRhal70"
      },
      "outputs": [],
      "source": [
        "# Tokenize the training data input\n",
        "def encode_code(comment_text):\n",
        "    inputs = tokenizer(comment_text, padding=True, truncation=True, max_length=256, return_tensors='tf')\n",
        "    outputs = model(inputs)[0]\n",
        "    return outputs.numpy()\n",
        "\n",
        "code_train = np.vstack(Dataset_train['comment_text'].apply(encode_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6fzqnvXbo1H",
        "outputId": "aa90f3bd-2c54-4dc6-889e-56cd98a56024"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "code_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5xFnaykiHOt"
      },
      "outputs": [],
      "source": [
        "# convert values to integer\n",
        "code_train = tf.cast(code_train , tf.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfe4mMUisPi4"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsbJCEbRpSD4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Compile our model\n",
        "# Lower learning rates are often better for fine-tuning transformers\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUAtMWprZX9V",
        "outputId": "9ae49ccf-88ad-4cb3-8833-6352159c7315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 73s 97ms/step - loss: 0.0274 - accuracy: 0.9740\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 14s 91ms/step - loss: 0.0281 - accuracy: 0.9776\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 16s 103ms/step - loss: 0.0296 - accuracy: 0.9758\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 22s 138ms/step - loss: 0.0248 - accuracy: 0.9764\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 13s 85ms/step - loss: 0.0345 - accuracy: 0.9770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d0579ae30>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# training the model\n",
        "model.fit(code_train, labels , epochs = 5 , batch_size= 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "iXWZ5DbnaIqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnesgnokMp5r"
      },
      "outputs": [],
      "source": [
        "# Tokenize the testing data input\n",
        "code_test = np.vstack(Dataset_test['comment_text'].apply(encode_code))\n",
        "code_test = tf.cast(code_test , tf.int32)\n",
        "code_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwXMRhLWOMnj"
      },
      "outputs": [],
      "source": [
        "# define testing label\n",
        "labels_test = np.array(Dataset_test[\"target\"])\n",
        "len(labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69_REQwFc5ka",
        "outputId": "7ce63c8b-98fd-4a68-89d0-523b7c4ab13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 4s 35ms/step\n"
          ]
        }
      ],
      "source": [
        "# Prediction of testing data\n",
        "result = model.predict(code_test)\n",
        "predicted_labels = np.argmax(result.logits, axis=1)  # taking value with highiger probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B1E7Uzldks9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a79c78b-ae9d-4bd5-f9a6-cc6faf56fe98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "len(predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4QKttabk_cH",
        "outputId": "08ff7f85-9f74-4765-8d12-bfb536ef140f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.713, precision: 0.5526720070229343, recall: 0.6499999999999999, f1score: 0.5328464301817016\n"
          ]
        }
      ],
      "source": [
        "predictions = predicted_labels\n",
        "labels =  labels_test\n",
        "# printing accuracy and other parameters\n",
        "precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "acc = accuracy_score(labels, predictions)\n",
        "print(f'acc: {acc}, precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speech to text input"
      ],
      "metadata": {
        "id": "fRzHWG35aR_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "API_TOKEN = \"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\"\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "# using pretrained model\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h\"\n",
        "\n",
        "def query(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n"
      ],
      "metadata": {
        "id": "agAe9hgEaRgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# song = requests.get(\"https://www.macaronisoup.com/songs/mp3/BINGO.mp3\")\n",
        "# file = open(\"BINGO.mp3\", \"wb\")\n",
        "# for chunk in song.iter_content(100000):\n",
        "#   file.write(chunk)\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "G-XRZmYfSGm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"content/BINGO.mp3\""
      ],
      "metadata": {
        "id": "yQ04X7F-DTPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = query(\"URL\")\n",
        "with open(\"URL\", \"rb\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "input = pipe(\"URL\")"
      ],
      "metadata": {
        "id": "1u0nEUBDahpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4794cfbb-5043-40d6-91a3-52117b59299c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input  = [input[\"text\"]]\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1c0tbQUTimG",
        "outputId": "3fe9f08f-569f-4e07-be61-3b0fee530bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'THERE WAS A FARMER HAD ADON ND BIGOE WAS HIS NAY U B A AN GT U A AN GY U B A ANGY UAN BINGOE WAS HIS AY MO B AY AND G O THERE WAS A BARBER'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "k86yc67nak0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = pd.DataFrame(input)\n",
        "input = input.rename(columns={0: 'comment_text'})\n",
        "# tokenize the input song\n",
        "input = np.vstack(input['comment_text'].apply(encode_code))\n",
        "input = tf.cast(input , tf.int32)\n",
        "input.shape"
      ],
      "metadata": {
        "id": "fbpB7ZdscAiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict output\n",
        "output = model.predict(input)\n",
        "output = np.argmax(output.logits, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiefVNzhc5Ey",
        "outputId": "acb28a18-6fb3-489b-c1c5-ca5169dd0147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if output is 1 then it is toxic other wise it is non-toxic\n",
        "if output[0] == 1:\n",
        "  print(\"toxic\")\n",
        "else:\n",
        "  print(\"Non-toxic\")"
      ],
      "metadata": {
        "id": "QJEFOz96dLBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}